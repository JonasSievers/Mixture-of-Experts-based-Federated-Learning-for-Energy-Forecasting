{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "\n",
    "from utils.modelgenerator import *\n",
    "from utils.modelhandler import *\n",
    "from utils.datahandler import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling\n",
    "- **Get Data**: Load data to dataframe from User5. Data includes Date, temp, hour and Electricity consumption\n",
    "- **Train, Validation and Test datasets**: Split Data into train, validation and test datasets\n",
    "- **Normalize Data**: Normalize each feature of the the different datasets\n",
    "- **Sequencing**: Take the 3 datasets and split them into sequences of length=sequence_length, then split the sequence into  X (features) and Y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User2</th>\n",
       "      <th>temp</th>\n",
       "      <th>rhum</th>\n",
       "      <th>wspd</th>\n",
       "      <th>PC1</th>\n",
       "      <th>hour sin</th>\n",
       "      <th>hour cos</th>\n",
       "      <th>User2_lag_24hrs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-07-08 00:00:00</th>\n",
       "      <td>0.215</td>\n",
       "      <td>9.8</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.453691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-08 01:00:00</th>\n",
       "      <td>0.136</td>\n",
       "      <td>9.8</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.453691</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-08 02:00:00</th>\n",
       "      <td>0.187</td>\n",
       "      <td>9.8</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.453691</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     User2  temp  rhum  wspd       PC1  hour sin  hour cos  \\\n",
       "Date                                                                         \n",
       "2012-07-08 00:00:00  0.215   9.8  85.0   0.0 -2.453691  0.000000  1.000000   \n",
       "2012-07-08 01:00:00  0.136   9.8  85.0   0.0 -2.453691  0.258819  0.965926   \n",
       "2012-07-08 02:00:00  0.187   9.8  85.0   0.0 -2.453691  0.500000  0.866025   \n",
       "\n",
       "                     User2_lag_24hrs  \n",
       "Date                                  \n",
       "2012-07-08 00:00:00            0.102  \n",
       "2012-07-08 01:00:00            0.116  \n",
       "2012-07-08 02:00:00            0.133  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get data \n",
    "cwd = os.path.normpath(os.getcwd())\n",
    "df = pd.read_csv(cwd+'/data/df_with_final_features.csv', index_col='Date') #df = pd.read_csv('user5.csv')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "#df = df[['User5', 'temp', 'rhum']]\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "#Select only 1 User for testing\n",
    "df = df[['User2', 'temp', 'rhum', 'wspd', 'PC1', 'hour sin', 'hour cos', 'User2_lag_24hrs']]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sclaing \n",
    "#Sequencing\n",
    "sequence_length = 49\n",
    "batch_size = 16\n",
    "\n",
    "dh = Datahandler()\n",
    "\n",
    "#Create Train, Validation and Test datasets\n",
    "n = len(df)\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]\n",
    "\n",
    "num_features = df.shape[1] #4\n",
    "\n",
    "\n",
    "train_df = dh.min_max_scaling(train_df)\n",
    "val_df = dh.min_max_scaling(val_df)\n",
    "test_df = dh.min_max_scaling(test_df)\n",
    "\n",
    "\n",
    "train_sequences = dh.create_sequences(train_df, sequence_length)\n",
    "val_sequences = dh.create_sequences(val_df, sequence_length)\n",
    "test_sequences = dh.create_sequences(test_df, sequence_length)\n",
    "\n",
    "X_train, y_train = dh.prepare_data(train_sequences, batch_size)\n",
    "X_val, y_val = dh.prepare_data(val_sequences, batch_size)\n",
    "X_test, y_test = dh.prepare_data(test_sequences, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "Build and test different model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Helper function for time callback to log training time per epoch\\n#Callback to logg model fitting time\\nfrom timeit import default_timer as timer\\n\\nclass timecallback(tf.keras.callbacks.Callback):\\n    def __init__(self, logs={}):\\n        self.logs=[]\\n    def on_epoch_begin(self, epoch, logs={}):\\n        self.starttime = timer()\\n    def on_epoch_end(self, epoch, logs={}):\\n        self.logs.append((epoch, timer()-self.starttime))\\n        \\ntimetaken = timecallback()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "# #All models\n",
    "horizon = 1\n",
    "max_epochs = 100\n",
    "m1 = ModelGenerator()\n",
    "mh = Modelhandler()\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics=[\n",
    "    tf.keras.metrics.RootMeanSquaredError(), \n",
    "    tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "    tf.keras.metrics.MeanAbsoluteError(),\n",
    "]\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='min')\n",
    "callbacks=[early_stopping]\n",
    "\"\"\"#Helper function for time callback to log training time per epoch\n",
    "#Callback to logg model fitting time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "class timecallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append((epoch, timer()-self.starttime))\n",
    "        \n",
    "timetaken = timecallback()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MoE\n",
    "num_experts = 5\n",
    "sequenze_length = 48\n",
    "expert_capacity = sequenze_length*batch_size // num_experts #= 48 / 10 = 4\n",
    "\n",
    "top_k = 3\n",
    "dense_1_units = 32\n",
    "expert_hidden_units = 16\n",
    "expert_output_units = 32\n",
    "dense_2_units = 32\n",
    "dense_3_units= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Gating\n",
    "\n",
    "#softgated_moe_model = m1.build_softgated_moe_model(X_train, batch_size, horizon, dense_1_units, num_experts, expert_hidden_units, expert_output_units, dense_2_units, m1)\n",
    "#moe_model_history  = mh.compile_fit_evaluate_model(softgated_moe_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\n",
    "#mh.plot_model_predictions(softgated_moe_model, moe_model_history, y_test, X_test, batch_size, plt_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_1_moe_model\n",
    "\n",
    "#top1_moe_model = m1.build_top1_moe_model(X_train, batch_size, horizon, sequenze_length, num_experts, expert_capacity, expert_dim, dense_1_units, dense_2_units, dense_3_units, m1)\n",
    "#history_dense_model  = mh.compile_fit_evaluate_model(top1_moe_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\n",
    "#mh.plot_model_predictions(top1_moe_model, history_dense_model, y_test, X_test, batch_size, plt_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_k_moe_model\n",
    "\n",
    "#topk_moe_model = m1.build_topk_moe_model(X_train, batch_size, horizon, dense_1_units, num_experts, top_k, expert_hidden_units, expert_output_units, dense_2_units, m1)\n",
    "#moe_model_history  = mh.compile_fit_evaluate_model(topk_moe_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\n",
    "#mh.plot_model_predictions(topk_moe_model, moe_model_history, y_test, X_test, batch_size, plt_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer:  (None, 48, 32)\n",
      "Transformer:  (None, 48, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#Build Deep Learning Models\\ndense_model = m1.build_dense_model(X_train, horizon, units=64)\\nlstm_model = m1.build_lstm_model(X_train, horizon, lstm_cells=64)\\ncnn_model = m1.build_cnn_model(X_train, horizon, filter=64, kernel_size=3)\\nbilstm_model = m1.build_bilstm_model(X_train, horizon, bilstm_cells=32)\\nprobability_model = m1.build_probability_model(X_train, horizon, units=128)\\ntransformer_model = m1.build_transformer_model(X_train, horizon, num_features, num_heads=2, key_dim=4)\\n\\n#Compile, fit, evaluate deep learning models\\nhistory_dense_model  = mh.compile_fit_evaluate_model(dense_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\\nhistory_lstm_model  = mh.compile_fit_evaluate_model(lstm_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\\nhistory_cnn_model  = mh.compile_fit_evaluate_model(cnn_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\\nhistory_bilstm_model  = mh.compile_fit_evaluate_model(bilstm_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\\nhistory_probability_model  = mh.compile_fit_evaluate_model(probability_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\\nhistory_transformer_model  = mh.compile_fit_evaluate_model(transformer_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\\n\\n\\n#Plot deep learning models\\nmh.plot_model_predictions(dense_model, history_dense_model, y_test, X_test, batch_size)\\nmh.plot_model_predictions(lstm_model, history_dense_model, y_test, X_test, batch_size)\\nmh.plot_model_predictions(cnn_model, history_dense_model, y_test, X_test, batch_size)\\nmh.plot_model_predictions(bilstm_model, history_dense_model, y_test, X_test, batch_size)\\nmh.plot_model_predictions(dense_model, history_probability_model, y_test, X_test, batch_size)\\nmh.plot_model_predictions(dense_model, history_transformer_model, y_test, X_test, batch_size)\\n\\n\\n#Statistical models\\nsvm_model = m1.build_svm_model(kernel='linear')\\nelasticnet_regression_model = m1.build_elasticnet_regression_model(alpha=0.0001, l1_ratio=0.5)\\ndecisiontree_model = m1.build_decisiontree_model()\\nrandomforrest_model = m1.build_randomforrest_model(n_estimators=10)\\nk_neighbors_model = m1.build_k_neighbors_model(n_neighbors=5)\\n\\n\\nmh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, decisiontree_model)\\nmh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, svm_model)\\nmh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, elasticnet_regression_model)\\nmh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, randomforrest_model)\\nmh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, k_neighbors_model)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Build Deep Learning Models\n",
    "dense_model = m1.build_dense_model(X_train, horizon, units=64)\n",
    "lstm_model = m1.build_lstm_model(X_train, horizon, lstm_cells=64)\n",
    "cnn_model = m1.build_cnn_model(X_train, horizon, filter=64, kernel_size=3)\n",
    "bilstm_model = m1.build_bilstm_model(X_train, horizon, bilstm_cells=32)\n",
    "probability_model = m1.build_probability_model(X_train, horizon, units=128)\n",
    "transformer_model = m1.build_transformer_model(X_train, horizon, num_features, num_heads=2, key_dim=4)\n",
    "\n",
    "#Compile, fit, evaluate deep learning models\n",
    "history_dense_model  = mh.compile_fit_evaluate_model(dense_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\n",
    "history_lstm_model  = mh.compile_fit_evaluate_model(lstm_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\n",
    "history_cnn_model  = mh.compile_fit_evaluate_model(cnn_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\n",
    "history_bilstm_model  = mh.compile_fit_evaluate_model(bilstm_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\n",
    "history_probability_model  = mh.compile_fit_evaluate_model(probability_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\n",
    "history_transformer_model  = mh.compile_fit_evaluate_model(transformer_model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks)\n",
    "\n",
    "\n",
    "#Plot deep learning models\n",
    "mh.plot_model_predictions(dense_model, history_dense_model, y_test, X_test, batch_size)\n",
    "mh.plot_model_predictions(lstm_model, history_dense_model, y_test, X_test, batch_size)\n",
    "mh.plot_model_predictions(cnn_model, history_dense_model, y_test, X_test, batch_size)\n",
    "mh.plot_model_predictions(bilstm_model, history_dense_model, y_test, X_test, batch_size)\n",
    "mh.plot_model_predictions(dense_model, history_probability_model, y_test, X_test, batch_size)\n",
    "mh.plot_model_predictions(dense_model, history_transformer_model, y_test, X_test, batch_size)\n",
    "\n",
    "\n",
    "#Statistical models\n",
    "svm_model = m1.build_svm_model(kernel='linear')\n",
    "elasticnet_regression_model = m1.build_elasticnet_regression_model(alpha=0.0001, l1_ratio=0.5)\n",
    "decisiontree_model = m1.build_decisiontree_model()\n",
    "randomforrest_model = m1.build_randomforrest_model(n_estimators=10)\n",
    "k_neighbors_model = m1.build_k_neighbors_model(n_neighbors=5)\n",
    "\n",
    "\n",
    "mh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, decisiontree_model)\n",
    "mh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, svm_model)\n",
    "mh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, elasticnet_regression_model)\n",
    "mh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, randomforrest_model)\n",
    "mh.statistical_model_compile_fit_evaluate(X_train, y_train, X_test, y_test, k_neighbors_model)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
