{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online STLF with data streams and drift detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Local STLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "#import sys  \n",
    "#sys.path.append(\"../\")  \n",
    "#from utils.modelgenerator import *\n",
    "#from utils.modelhandler import *\n",
    "#from utils.datahandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#min_max_scaling\n",
    "#Sclaes all columns of the dataframe df to the rang (0,1)\n",
    "def min_max_scaling(df): #normailizing\n",
    "    #Min Max Sclaing\n",
    "    col_names = df.columns\n",
    "    features = df[col_names]\n",
    "    scaler = MinMaxScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    df_scaled = pd.DataFrame(features, columns = col_names, index=df.index)\n",
    "    return df_scaled\n",
    "\n",
    "#create_sequences\n",
    "#Split the dataframe into datasets with sequences of lngth=Sequence_length\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(df) - sequence_length + 1):\n",
    "        sequence = df.iloc[i:i+sequence_length, :]  # Take all columns\n",
    "        sequences.append(sequence.values)\n",
    "    return np.array(sequences)\n",
    "\n",
    "#prepare_data\n",
    "# Split each sequence into X (features) and Y (labels). \n",
    "# The label Y must be the FIRST column! The last batch is discarded, when < batch_size\n",
    "def prepare_data(sequences, batch_size):\n",
    "    X = sequences[:, :-1, :].astype('float32') #For all sequences, Exclude last row of the sequence, take all columns\n",
    "    y = sequences[:, -1, 0].astype('float32') #For all sequences, Take the last row of the sequence, take the first column\n",
    "\n",
    "    #As some models need to reshape the inputs, the correct batch_size is important\n",
    "    #Adjust the dataset_size to be divisible by batch_size by discarding the remaining data points not fitting a complete batch.\n",
    "    num_batches = len(X) // batch_size\n",
    "    adjusted_X = X[:num_batches * batch_size]\n",
    "    adjusted_y = y[:num_batches * batch_size]\n",
    "\n",
    "    return adjusted_X, adjusted_y\n",
    "\n",
    "class TimingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "\n",
    "    def get_training_times_df(self):\n",
    "        total_training_time = time.time() - self.start_time\n",
    "        average_epoch_times = [sum(self.epoch_times[:i+1]) / (i + 1) for i in range(len(self.epoch_times))]\n",
    "        data = {\n",
    "            'Epoch': list(range(1, len(self.epoch_times) + 1)),\n",
    "            'Epoch Train_time': self.epoch_times,\n",
    "            'Epoch Avg Train_time': average_epoch_times,\n",
    "            'Total Training Time': total_training_time\n",
    "        }\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        self.epoch_times = []\n",
    "        self.losses = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'test_loss': []\n",
    "        }\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "\n",
    "        self.losses['epoch'].append(epoch)\n",
    "        self.losses['train_loss'].append(logs['loss'])\n",
    "        self.losses['val_loss'].append(logs['val_loss'])\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        self.losses['test_loss'].append(logs['loss'])\n",
    "\n",
    "    def get_loss_df(self):\n",
    "        total_training_time = time.time() - self.start_time\n",
    "        average_epoch_times = [sum(self.epoch_times[:i+1]) / (i + 1) for i in range(len(self.epoch_times))]\n",
    "        self.losses['avg_epoch_time'] = average_epoch_times\n",
    "        self.losses['total_training_time'] = total_training_time\n",
    "        return pd.DataFrame(self.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for models\n",
    "from keras import layers, models\n",
    "\n",
    "def build_dense_model(X_train, horizon, num_layers, units, batch_size):\n",
    "\n",
    "    input_data = layers.Input(shape=(X_train.shape[1], X_train.shape[2]), batch_size=batch_size) \n",
    "    x =  layers.Dense(units, activation='relu')(input_data)\n",
    "    for _ in range(num_layers-1):\n",
    "        x = layers.Dense(units, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    output = layers.Dense(horizon)(x) \n",
    "\n",
    "    dense_model = tf.keras.Model(inputs=input_data, outputs=output, name=\"Dense_model\")\n",
    "\n",
    "    return dense_model\n",
    "\n",
    "#This method compiles the model using Adam optimizer, fits the model, and evaluates it\n",
    "def compile_fit_evaluate_model(model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks, user= \"\", hyper=\"\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)):\n",
    "    #Compile the model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=callbacks, verbose=0,)\n",
    "    #model = tf.keras.models.load_model('models/best_model.h5')\n",
    "    #Evaluate the model on test dataset\n",
    "    test_loss = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    train_times = callbacks[1].get_training_times_df()\n",
    "    total_train_time = train_times[\"Total Training Time\"][0]\n",
    "    avg_time_epoch = train_times[\"Epoch Avg Train_time\"].iloc[-1]\n",
    "\n",
    "    model_user_result = pd.DataFrame(\n",
    "        data=[[user, hyper, total_train_time, avg_time_epoch, test_loss[0], test_loss[1], test_loss[2], test_loss[3]]], \n",
    "        columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"]\n",
    "    )\n",
    "\n",
    "    return history, model_user_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data \n",
    "cwd = os.path.normpath(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "df = pd.read_csv(cwd+'/data/2feature_engineering_data/df_with_final_features.csv', index_col='Date') #df = pd.read_csv('user5.csv')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "#df = df[['User5', 'temp', 'rhum']]\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df_array = []\n",
    "for idx in range(1):\n",
    "    df_array.append(df[[f'User{idx+1}', 'temp', 'rhum', 'wspd', 'PC1', 'hour sin', 'hour cos', f'User{idx+1}_lag_24hrs']])\n",
    "\n",
    "#df_array[3].head(3)\n",
    "\n",
    "#Train, Validation and Test datasets\n",
    "sequence_length = 25\n",
    "batch_size = 16\n",
    "num_features = df_array[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting\\src\\datastreams\\local_learning_with_data_streams.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m test_df \u001b[39m=\u001b[39m df[\u001b[39mint\u001b[39m(n\u001b[39m*\u001b[39m\u001b[39m0.9\u001b[39m):]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Min max sclaing\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_df \u001b[39m=\u001b[39m min_max_scaling(\u001b[39mself\u001b[39;49m, train_df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m val_df \u001b[39m=\u001b[39m min_max_scaling(\u001b[39mself\u001b[39m, val_df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_df \u001b[39m=\u001b[39m min_max_scaling(\u001b[39mself\u001b[39m, test_df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "#Create Train, Validation and Test datasets\n",
    "for idx, df in enumerate(df_array):\n",
    "    n = len(df)\n",
    "    train_df = df[0:int(n*0.7)]\n",
    "    val_df = df[int(n*0.7):int(n*0.9)]\n",
    "    test_df = df[int(n*0.9):]\n",
    "\n",
    "    # Min max sclaing\n",
    "    train_df = min_max_scaling(train_df)\n",
    "    val_df = min_max_scaling(val_df)\n",
    "    test_df = min_max_scaling(test_df)\n",
    "\n",
    "    # Sequencing\n",
    "    train_sequences = create_sequences(train_df, sequence_length)\n",
    "    val_sequences = create_sequences(val_df, sequence_length)\n",
    "    test_sequences = create_sequences(test_df, sequence_length)\n",
    "\n",
    "    #Split into feature and label\n",
    "    X_train[f'user{idx+1}'], y_train[f'user{idx+1}'] = prepare_data(train_sequences, batch_size)\n",
    "    X_val[f'user{idx+1}'], y_val[f'user{idx+1}'] = prepare_data(val_sequences, batch_size)\n",
    "    X_test[f'user{idx+1}'], y_test[f'user{idx+1}'] = prepare_data(test_sequences, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:585: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#General Hyperparameters\n",
    "# #All models\n",
    "horizon = 1\n",
    "max_epochs = 100\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics=[\n",
    "    tf.keras.metrics.RootMeanSquaredError(), \n",
    "    tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "    tf.keras.metrics.MeanAbsoluteError(),\n",
    "]\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='min')\n",
    "timing_callback = TimingCallback()\n",
    "custom_callback = CustomCallback()\n",
    "#model_checkpoint = ModelCheckpoint('models/best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks=[early_stopping, timing_callback, custom_callback] #model_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_results = pd.DataFrame(columns=['architecture', 'train_time', 'avg_time_epoch', 'mse','mse_std', 'rmse','rmse_std','mape','mape_std','mae','mae_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rs1044\\Documents\\GitHub\\Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting\\src\\datastreams\\local_learning_with_data_streams.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUser: \u001b[39m\u001b[39m\"\u001b[39m, idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mround\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m#print(\"Round: \", round)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     dense_model \u001b[39m=\u001b[39m build_dense_model(X_train[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m], horizon, num_layers\u001b[39m=\u001b[39mdense_layers, units\u001b[39m=\u001b[39mdense_units, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dense_histroy, dense_user_results \u001b[39m=\u001b[39m compile_fit_evaluate_model(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         model\u001b[39m=\u001b[39mdense_model, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         loss\u001b[39m=\u001b[39mloss, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rs1044/Documents/GitHub/Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting/src/datastreams/local_learning_with_data_streams.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m# Add the 'architecture' column from dense_user_results to dense_results\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Dense 1 -------------------------------------------------------------\n",
    "\n",
    "#Dense Hyperparameter\n",
    "dense_architecture = \"L3_U16\"\n",
    "dense_layers = 3\n",
    "dense_units = 16\n",
    "dense_all_results = pd.DataFrame(columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"])\n",
    "\n",
    "#For each of the 3 user\n",
    "for idx in range(len(df_array)):\n",
    "    print(\"User: \", idx+1)\n",
    "    for round in range(3):\n",
    "        #print(\"Round: \", round)\n",
    "        dense_model = build_dense_model(X_train[f'user{idx+1}'], horizon, num_layers=dense_layers, units=dense_units, batch_size=batch_size)\n",
    "        dense_histroy, dense_user_results = compile_fit_evaluate_model(\n",
    "            model=dense_model, \n",
    "            loss=loss, \n",
    "            metrics=metrics, \n",
    "            X_train=X_train[f'user{idx+1}'],\n",
    "            y_train = y_train[f'user{idx+1}'], \n",
    "            max_epochs = max_epochs, \n",
    "            batch_size=batch_size, \n",
    "            X_val=X_val[f'user{idx+1}'], \n",
    "            y_val=y_val[f'user{idx+1}'], \n",
    "            X_test=X_test[f'user{idx+1}'], \n",
    "            y_test=y_test[f'user{idx+1}'], \n",
    "            callbacks=callbacks, \n",
    "            user=f'user{idx+1}', \n",
    "            hyper=dense_architecture,\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        )\n",
    "        # Add the 'architecture' column from dense_user_results to dense_results\n",
    "        dense_all_results = pd.merge(dense_all_results, dense_user_results, how='outer')   \n",
    "\n",
    "    #dense_model.save(cwd + f\"/models/Local_learning/Dense/{dense_architecture}/User{idx}\")\n",
    "    print(\"Saved Soft_Dense_MoE\")  \n",
    "\n",
    "\n",
    "for idx in range(len(df_array)):\n",
    "    new_row = {\n",
    "        'architecture': dense_architecture,\n",
    "        'train_time': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"train_time\"].mean(), \n",
    "        'avg_time_epoch' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"avg_time_epoch\"].mean(),\n",
    "        'mse': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mse\"].mean(),\n",
    "        'mse_std' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mse\"].std(),\n",
    "        'rmse': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"rmse\"].mean(),\n",
    "        'rmse_std' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"rmse\"].std(),\n",
    "        'mape': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mape\"].mean(),\n",
    "        'mape_std' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mape\"].std(),\n",
    "        'mae': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mae\"].mean(),\n",
    "        'mae_std' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mae\"].std(),\n",
    "    }\n",
    "    dense_results.loc[len(dense_results)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_time</th>\n",
       "      <th>avg_time_epoch</th>\n",
       "      <th>mse</th>\n",
       "      <th>mse_std</th>\n",
       "      <th>rmse</th>\n",
       "      <th>rmse_std</th>\n",
       "      <th>mape</th>\n",
       "      <th>mape_std</th>\n",
       "      <th>mae</th>\n",
       "      <th>mae_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L3_U16</td>\n",
       "      <td>13.438615</td>\n",
       "      <td>0.420715</td>\n",
       "      <td>0.03389</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.184058</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>142224.630208</td>\n",
       "      <td>20025.937011</td>\n",
       "      <td>0.12693</td>\n",
       "      <td>0.001977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  architecture  train_time  avg_time_epoch      mse   mse_std      rmse  \\\n",
       "0       L3_U16   13.438615        0.420715  0.03389  0.001599  0.184058   \n",
       "\n",
       "   rmse_std           mape      mape_std      mae   mae_std  \n",
       "0  0.004374  142224.630208  20025.937011  0.12693  0.001977  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_results.to_csv(f'../../evaluations/Test_Datastream_Dense.csv')\n",
    "dense_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User1</th>\n",
       "      <th>User10</th>\n",
       "      <th>User11</th>\n",
       "      <th>User12</th>\n",
       "      <th>User13</th>\n",
       "      <th>User14</th>\n",
       "      <th>User15</th>\n",
       "      <th>User16</th>\n",
       "      <th>User17</th>\n",
       "      <th>User18</th>\n",
       "      <th>...</th>\n",
       "      <th>User6</th>\n",
       "      <th>User7</th>\n",
       "      <th>User8</th>\n",
       "      <th>User9</th>\n",
       "      <th>temp</th>\n",
       "      <th>dwpt</th>\n",
       "      <th>rhum</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-07-01 00:00:00</th>\n",
       "      <td>0.068</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.012</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-01 01:00:00</th>\n",
       "      <td>0.786</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.022</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-01 02:00:00</th>\n",
       "      <td>0.544</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.023</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-01 03:00:00</th>\n",
       "      <td>0.612</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1014.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-01 04:00:00</th>\n",
       "      <td>0.665</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.038</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1014.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     User1  User10  User11  User12  User13  User14  User15  \\\n",
       "Date                                                                         \n",
       "2012-07-01 00:00:00  0.068   0.703   0.353   0.210   0.138   0.208   0.065   \n",
       "2012-07-01 01:00:00  0.786   0.036   0.547   0.197   0.343   0.176   0.067   \n",
       "2012-07-01 02:00:00  0.544   0.045   0.519   0.163   0.339   0.164   0.057   \n",
       "2012-07-01 03:00:00  0.612   0.031   0.324   0.173   0.337   0.178   0.063   \n",
       "2012-07-01 04:00:00  0.665   0.018   0.343   0.156   0.363   0.193   0.065   \n",
       "\n",
       "                     User16  User17  User18  ...  User6  User7  User8  User9  \\\n",
       "Date                                         ...                               \n",
       "2012-07-01 00:00:00   0.340   0.129   0.576  ...  0.636  0.106  0.156  0.012   \n",
       "2012-07-01 01:00:00   0.508   0.121   0.128  ...  0.253  0.098  0.151  0.022   \n",
       "2012-07-01 02:00:00   0.542   0.141   0.098  ...  0.220  0.089  0.152  0.023   \n",
       "2012-07-01 03:00:00   0.590   0.165   0.097  ...  0.241  0.103  0.148  0.012   \n",
       "2012-07-01 04:00:00   0.551   0.122   0.101  ...  0.199  0.090  0.146  0.038   \n",
       "\n",
       "                     temp  dwpt  rhum   wdir  wspd    pres  \n",
       "Date                                                        \n",
       "2012-07-01 00:00:00   5.1   4.5  96.0    NaN   0.0  1015.2  \n",
       "2012-07-01 01:00:00   5.1   4.5  96.0    NaN   0.0  1015.2  \n",
       "2012-07-01 02:00:00   5.1   4.5  96.0    NaN   0.0  1015.2  \n",
       "2012-07-01 03:00:00   2.7   2.3  97.0  200.0   7.6  1014.8  \n",
       "2012-07-01 04:00:00   2.7   2.3  97.0  200.0   7.6  1014.8  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import stream\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "cwd = os.path.normpath(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "df = pd.read_csv(cwd+'/data/3final_data/final_dataset.csv', index_col='Date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': datetime.datetime(2012, 7, 1, 0, 0), 'User10': '0.703', 'User11': '0.353', 'User12': '0.21', 'User13': '0.138', 'User14': '0.208', 'User15': '0.065', 'User16': '0.34', 'User17': '0.129', 'User18': '0.576', 'User19': '0.075', 'User2': '0.254', 'User20': '2.35', 'User21': '0.174', 'User22': '0.698', 'User23': '0.048', 'User24': '0.163', 'User25': '0.175', 'User26': '0.888', 'User27': '0.818', 'User28': '0.138', 'User29': '0.154', 'User3': '1.048', 'User30': '0.022', 'User31': '0.288', 'User32': '0.057', 'User33': '0.284', 'User34': '0.244', 'User35': '0.397', 'User36': '0.102', 'User4': '0.09', 'User5': '0.098', 'User6': '0.636', 'User7': '0.106', 'User8': '0.156', 'User9': '0.012', 'temp': 5.1, 'dwpt': 4.5, 'rhum': 96.0, 'wdir': '', 'wspd': 0.0, 'pres': 1015.2} 0.068\n"
     ]
    }
   ],
   "source": [
    "for x, y in stream.iter_csv(\n",
    "    cwd+'/data/3final_data/final_dataset.csv',\n",
    "    converters={\n",
    "        'User1': float,\n",
    "        'temp': float,\n",
    "        'dwpt': float,\n",
    "        'rhum': float,\n",
    "        #'wdir': float,\n",
    "        'wspd': float,\n",
    "        'pres': float\n",
    "    },\n",
    "    parse_dates={'Date': '%Y-%m-%d %H:%M:%S'},\n",
    "    target='User1'\n",
    "):\n",
    "    print(x,y)\n",
    "    break\n",
    "\n",
    "#x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Drift detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import drift\n",
    "\n",
    "adwin = drift.ADWIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change detected at index 1023, input value: 4\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from river import drift\n",
    "\n",
    "rng = random.Random(12345)\n",
    "adwin = drift.ADWIN()\n",
    "\n",
    "data_stream = rng.choices([0, 1], k=1000) + rng.choices(range(4, 8), k=1000)\n",
    "\n",
    "for i, val in enumerate(data_stream):\n",
    "    _ = adwin.update(val)\n",
    "    if adwin.drift_detected:\n",
    "        print(f\"Change detected at index {i}, input value: {val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
