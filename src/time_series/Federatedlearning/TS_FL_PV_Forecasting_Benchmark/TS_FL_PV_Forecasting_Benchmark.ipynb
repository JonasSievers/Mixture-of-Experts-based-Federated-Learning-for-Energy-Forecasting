{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import sys  \n",
    "sys.path.append(\"../../../\")  \n",
    "from utils.models import *\n",
    "from utils.datahandling import *\n",
    "from utils.modelrunner import *\n",
    "from utils.federatedrunner import *\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "logging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data \n",
    "num_users = 30\n",
    "\n",
    "cwd = os.path.normpath(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))))\n",
    "df = pd.read_csv(cwd+'/data/3final_data/Final_PV_dataset.csv', index_col='Date')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df_array = []\n",
    "for idx in range(num_users):\n",
    "    df_array.append(df[[f'User{idx+1}', 'temp', 'rhum', 'wspd', 'PC1', 'hour sin', 'hour cos', f'User{idx+1}_lag_24hrs']])\n",
    "\n",
    "df_array[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "sequence_length = 25\n",
    "batch_size = 16\n",
    "num_features = df_array[0].shape[1]\n",
    "horizon = 1\n",
    "max_epochs = 100\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics=[\n",
    "    tf.keras.metrics.RootMeanSquaredError(), \n",
    "    tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "    tf.keras.metrics.MeanAbsoluteError(),\n",
    "]\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='min')\n",
    "timing_callback = TimingCallback()\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "callbacks=[early_stopping, timing_callback, custom_callback]\n",
    "\n",
    "#BiLSTM\n",
    "lstm_architecture = \"bilstm\"\n",
    "lstm_layers = 2\n",
    "lstm_units = 8\n",
    "lstm_all_results = pd.DataFrame(columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"])\n",
    "lstm_results = pd.DataFrame(columns=['architecture', 'train_time', 'avg_time_epoch', 'mse','mse_std', 'rmse','rmse_std','mape','mape_std','mae','mae_std'])\n",
    "\n",
    "#CNN\n",
    "cnn_architecture = \"CNN_Model\"\n",
    "cnn_layers = 4\n",
    "cnn_kernel_size = 1\n",
    "cnn_filter_size = 8\n",
    "cnn_dense_units = 16\n",
    "cnn_all_results = pd.DataFrame(columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"])\n",
    "cnn_results = pd.DataFrame(columns=['architecture', 'train_time', 'avg_time_epoch', 'mse','mse_std', 'rmse','rmse_std','mape','mape_std','mae','mae_std'])\n",
    "\n",
    "#Transfotransformer_architecture = \"Transformer_ED2_h4_d32\"\n",
    "transformer_layers = 2\n",
    "transformer_heads = 4\n",
    "transformer_dense_units = 16\n",
    "transformer_all_results = pd.DataFrame(columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"])\n",
    "transformer_results = pd.DataFrame(columns=['architecture', 'train_time', 'avg_time_epoch', 'mse','mse_std', 'rmse','rmse_std','mape','mape_std','mae','mae_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, Validation and Test datasets\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "#Create Train, Validation and Test datasets\n",
    "for idx, df in enumerate(df_array):\n",
    "    n = len(df)\n",
    "    train_df = df[0:int(n*0.7)]\n",
    "    val_df = df[int(n*0.7):int(n*0.9)]\n",
    "    test_df = df[int(n*0.9):]\n",
    "\n",
    "    # Min max sclaing\n",
    "    train_df = min_max_scaling(train_df)\n",
    "    val_df = min_max_scaling(val_df)\n",
    "    test_df = min_max_scaling(test_df)\n",
    "\n",
    "    # Sequencing\n",
    "    train_sequences = create_sequences(train_df, sequence_length)\n",
    "    val_sequences = create_sequences(val_df, sequence_length)\n",
    "    test_sequences = create_sequences(test_df, sequence_length)\n",
    "\n",
    "    #Split into feature and label\n",
    "    X_train[f'user{idx+1}'], y_train[f'user{idx+1}'] = prepare_data(train_sequences, batch_size)\n",
    "    X_val[f'user{idx+1}'], y_val[f'user{idx+1}'] = prepare_data(val_sequences, batch_size)\n",
    "    X_test[f'user{idx+1}'], y_test[f'user{idx+1}'] = prepare_data(test_sequences, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Learning for Benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_rounds = 2\n",
    "num_rounds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.loadtxt(f'../../../../data/3final_data/Clusters_KMeans10_dtw.csv', delimiter=',').astype(int)\n",
    "num_clusters = 10\n",
    "cluster_users = {i: [] for i in range(num_clusters)}\n",
    "\n",
    "# Iterate through each cluster\n",
    "for cluster_number in range(num_clusters):\n",
    "    users_in_cluster = np.where(y == cluster_number)[0] +1\n",
    "    cluster_users[cluster_number] = users_in_cluster\n",
    "cluster_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
