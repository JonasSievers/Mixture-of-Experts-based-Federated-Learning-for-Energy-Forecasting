{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import sys  \\nsys.path.append(\"../../\")  \\nfrom utils.modelgenerator import *\\nfrom utils.modelhandler import *\\nfrom utils.datahandler import *'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\"\"\"import sys  \n",
    "sys.path.append(\"../../\")  \n",
    "from utils.modelgenerator import *\n",
    "from utils.modelhandler import *\n",
    "from utils.datahandler import *\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_max_scaling\n",
    "#Sclaes all columns of the dataframe df to the rang (0,1)\n",
    "def min_max_scaling(df): #normailizing\n",
    "    #Min Max Sclaing\n",
    "    col_names = df.columns\n",
    "    features = df[col_names]\n",
    "    scaler = MinMaxScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    df_scaled = pd.DataFrame(features, columns = col_names, index=df.index)\n",
    "    return df_scaled\n",
    "\n",
    "#create_sequences\n",
    "#Split the dataframe into datasets with sequences of lngth=Sequence_length\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(df) - sequence_length + 1):\n",
    "        sequence = df.iloc[i:i+sequence_length, :]  # Take all columns\n",
    "        sequences.append(sequence.values)\n",
    "    return np.array(sequences)\n",
    "\n",
    "#prepare_data\n",
    "# Split each sequence into X (features) and Y (labels). \n",
    "# The label Y must be the FIRST column! The last batch is discarded, when < batch_size\n",
    "def prepare_data(sequences, batch_size):\n",
    "    X = sequences[:, :-1, :].astype('float32') #For all sequences, Exclude last row of the sequence, take all columns\n",
    "    y = sequences[:, -1, 0].astype('float32') #For all sequences, Take the last row of the sequence, take the first column\n",
    "\n",
    "    #As some models need to reshape the inputs, the correct batch_size is important\n",
    "    #Adjust the dataset_size to be divisible by batch_size by discarding the remaining data points not fitting a complete batch.\n",
    "    num_batches = len(X) // batch_size\n",
    "    adjusted_X = X[:num_batches * batch_size]\n",
    "    adjusted_y = y[:num_batches * batch_size]\n",
    "\n",
    "    return adjusted_X, adjusted_y\n",
    "\n",
    "class TimingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "\n",
    "    def get_training_times_df(self):\n",
    "        total_training_time = time.time() - self.start_time\n",
    "        average_epoch_times = [sum(self.epoch_times[:i+1]) / (i + 1) for i in range(len(self.epoch_times))]\n",
    "        data = {\n",
    "            'Epoch': list(range(1, len(self.epoch_times) + 1)),\n",
    "            'Epoch Train_time': self.epoch_times,\n",
    "            'Epoch Avg Train_time': average_epoch_times,\n",
    "            'Total Training Time': total_training_time\n",
    "        }\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        self.epoch_times = []\n",
    "        self.losses = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'test_loss': []\n",
    "        }\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "\n",
    "        self.losses['epoch'].append(epoch)\n",
    "        self.losses['train_loss'].append(logs['loss'])\n",
    "        self.losses['val_loss'].append(logs['val_loss'])\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        self.losses['test_loss'].append(logs['loss'])\n",
    "\n",
    "    def get_loss_df(self):\n",
    "        total_training_time = time.time() - self.start_time\n",
    "        average_epoch_times = [sum(self.epoch_times[:i+1]) / (i + 1) for i in range(len(self.epoch_times))]\n",
    "        self.losses['avg_epoch_time'] = average_epoch_times\n",
    "        self.losses['total_training_time'] = total_training_time\n",
    "        return pd.DataFrame(self.losses)\n",
    "    \n",
    "#Helper functions for models\n",
    "from keras import layers, models\n",
    "\n",
    "def build_dense_model(X_train, horizon, num_layers, units, batch_size):\n",
    "\n",
    "    input_data = layers.Input(shape=(X_train.shape[1], X_train.shape[2]), batch_size=batch_size) \n",
    "    x =  layers.Dense(units, activation='relu')(input_data)\n",
    "    for _ in range(num_layers-1):\n",
    "        x = layers.Dense(units, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    output = layers.Dense(horizon)(x) \n",
    "\n",
    "    dense_model = tf.keras.Model(inputs=input_data, outputs=output, name=\"Dense_model\")\n",
    "\n",
    "    return dense_model\n",
    "\n",
    "#This method compiles the model using Adam optimizer, fits the model, and evaluates it\n",
    "def compile_fit_evaluate_model(model, loss, metrics, X_train, y_train, max_epochs, batch_size, X_val, y_val, X_test, y_test, callbacks, user= \"\", hyper=\"\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)):\n",
    "    #Compile the model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=callbacks, verbose=0,)\n",
    "    #model = tf.keras.models.load_model('models/best_model.h5')\n",
    "    #Evaluate the model on test dataset\n",
    "    test_loss = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    train_times = callbacks[1].get_training_times_df()\n",
    "    total_train_time = train_times[\"Total Training Time\"][0]\n",
    "    avg_time_epoch = train_times[\"Epoch Avg Train_time\"].iloc[-1]\n",
    "\n",
    "    model_user_result = pd.DataFrame(\n",
    "        data=[[user, hyper, total_train_time, avg_time_epoch, test_loss[0], test_loss[1], test_loss[2], test_loss[3]]], \n",
    "        columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"]\n",
    "    )\n",
    "\n",
    "    return history, model_user_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User1</th>\n",
       "      <th>temp</th>\n",
       "      <th>rhum</th>\n",
       "      <th>wspd</th>\n",
       "      <th>PC1</th>\n",
       "      <th>hour sin</th>\n",
       "      <th>hour cos</th>\n",
       "      <th>User1_lag_24hrs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-07-02 00:00:00</th>\n",
       "      <td>0.111</td>\n",
       "      <td>2.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.641741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-02 01:00:00</th>\n",
       "      <td>0.346</td>\n",
       "      <td>2.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.641741</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-02 02:00:00</th>\n",
       "      <td>0.079</td>\n",
       "      <td>2.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.641741</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     User1  temp  rhum  wspd       PC1  hour sin  hour cos  \\\n",
       "Date                                                                         \n",
       "2010-07-02 00:00:00  0.111   2.5  92.0   0.0 -2.641741  0.000000  1.000000   \n",
       "2010-07-02 01:00:00  0.346   2.5  92.0   0.0 -2.641741  0.258819  0.965926   \n",
       "2010-07-02 02:00:00  0.079   2.5  92.0   0.0 -2.641741  0.500000  0.866025   \n",
       "\n",
       "                     User1_lag_24hrs  \n",
       "Date                                  \n",
       "2010-07-02 00:00:00            0.125  \n",
       "2010-07-02 01:00:00            0.471  \n",
       "2010-07-02 02:00:00            0.121  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get data \n",
    "cwd = os.path.normpath(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))\n",
    "df = pd.read_csv(cwd+'/data/3final_data/Final_Grossload_dataset.csv', index_col='Date') #df = pd.read_csv('user5.csv')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "#df = df[['User5', 'temp', 'rhum']]\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df_array = []\n",
    "for idx in range(1):\n",
    "    df_array.append(df[[f'User{idx+1}', 'temp', 'rhum', 'wspd', 'PC1', 'hour sin', 'hour cos', f'User{idx+1}_lag_24hrs']])\n",
    "\n",
    "df_array[0].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, Validation and Test datasets\n",
    "sequence_length = 25\n",
    "batch_size = 16\n",
    "num_features = df_array[0].shape[1]\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "#Create Train, Validation and Test datasets\n",
    "for idx, df in enumerate(df_array):\n",
    "    n = len(df)\n",
    "    train_df = df[0:int(n*0.7)]\n",
    "    val_df = df[int(n*0.7):int(n*0.9)]\n",
    "    test_df = df[int(n*0.9):]\n",
    "\n",
    "    # Min max sclaing\n",
    "    train_df = min_max_scaling(train_df)\n",
    "    val_df = min_max_scaling(val_df)\n",
    "    test_df = min_max_scaling(test_df)\n",
    "\n",
    "    # Sequencing\n",
    "    train_sequences = create_sequences(train_df, sequence_length)\n",
    "    val_sequences = create_sequences(val_df, sequence_length)\n",
    "    test_sequences = create_sequences(test_df, sequence_length)\n",
    "\n",
    "    #Split into feature and label\n",
    "    X_train[f'user{idx+1}'], y_train[f'user{idx+1}'] = prepare_data(train_sequences, batch_size)\n",
    "    X_val[f'user{idx+1}'], y_val[f'user{idx+1}'] = prepare_data(val_sequences, batch_size)\n",
    "    X_test[f'user{idx+1}'], y_test[f'user{idx+1}'] = prepare_data(test_sequences, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:585: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#General Hyperparameters\n",
    "# #All models\n",
    "horizon = 1\n",
    "max_epochs = 100\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics=[\n",
    "    tf.keras.metrics.RootMeanSquaredError(), \n",
    "    tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "    tf.keras.metrics.MeanAbsoluteError(),\n",
    "]\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='min')\n",
    "timing_callback = TimingCallback()\n",
    "custom_callback = CustomCallback()\n",
    "#model_checkpoint = ModelCheckpoint('models/best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks=[early_stopping, timing_callback, custom_callback] #model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_results = pd.DataFrame(columns=['architecture', 'train_time', 'avg_time_epoch', 'mse','mse_std', 'rmse','rmse_std','mape','mape_std','mae','mae_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  1\n",
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Saved Soft_Dense_MoE\n"
     ]
    }
   ],
   "source": [
    "#Dense 1 -------------------------------------------------------------\n",
    "\n",
    "#Dense Hyperparameter\n",
    "dense_architecture = \"L3_U16\"\n",
    "dense_layers = 3\n",
    "dense_units = 16\n",
    "dense_all_results = pd.DataFrame(columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"])\n",
    "\n",
    "#For each of the users\n",
    "for idx in range(len(df_array)):\n",
    "    print(\"User: \", idx+1)\n",
    "    for round in range(5):\n",
    "        #print(\"Round: \", round)\n",
    "        dense_model = build_dense_model(X_train[f'user{idx+1}'], horizon, num_layers=dense_layers, units=dense_units, batch_size=batch_size)\n",
    "        dense_histroy, dense_user_results = compile_fit_evaluate_model(\n",
    "            model=dense_model, \n",
    "            loss=loss, \n",
    "            metrics=metrics, \n",
    "            X_train=X_train[f'user{idx+1}'],\n",
    "            y_train = y_train[f'user{idx+1}'], \n",
    "            max_epochs = max_epochs, \n",
    "            batch_size=batch_size, \n",
    "            X_val=X_val[f'user{idx+1}'], \n",
    "            y_val=y_val[f'user{idx+1}'], \n",
    "            X_test=X_test[f'user{idx+1}'], \n",
    "            y_test=y_test[f'user{idx+1}'], \n",
    "            callbacks=callbacks, \n",
    "            user=f'user{idx+1}', \n",
    "            hyper=dense_architecture,\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        )\n",
    "        # Add the 'architecture' column from dense_user_results to dense_results\n",
    "        dense_all_results = pd.merge(dense_all_results, dense_user_results, how='outer')   \n",
    "\n",
    "    #dense_model.save(cwd + f\"/models/Local_learning/Dense/{dense_architecture}/User{idx}\")\n",
    "    print(\"Saved Soft_Dense_MoE\")  \n",
    "\n",
    "\n",
    "for idx in range(len(df_array)):\n",
    "    new_row = {\n",
    "        'architecture': dense_architecture,\n",
    "        'train_time': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"train_time\"].mean(), \n",
    "        'avg_time_epoch' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"avg_time_epoch\"].mean(),\n",
    "        'mse': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mse\"].mean(),\n",
    "        'mse_std' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mse\"].std(),\n",
    "        'rmse': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"rmse\"].mean(),\n",
    "        'rmse_std' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"rmse\"].std(),\n",
    "        'mape': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mape\"].mean(),\n",
    "        'mape_std' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mape\"].std(),\n",
    "        'mae': dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mae\"].mean(),\n",
    "        'mae_std' : dense_all_results[dense_all_results[\"user\"]==f\"user{idx+1}\"][\"mae\"].std(),\n",
    "    }\n",
    "    dense_results.loc[len(dense_results)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>architecture</th>\n",
       "      <th>train_time</th>\n",
       "      <th>avg_time_epoch</th>\n",
       "      <th>mse</th>\n",
       "      <th>mse_std</th>\n",
       "      <th>rmse</th>\n",
       "      <th>rmse_std</th>\n",
       "      <th>mape</th>\n",
       "      <th>mape_std</th>\n",
       "      <th>mae</th>\n",
       "      <th>mae_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L3_U16</td>\n",
       "      <td>23.917387</td>\n",
       "      <td>1.121345</td>\n",
       "      <td>0.020921</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.144634</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>35257.523047</td>\n",
       "      <td>3342.083789</td>\n",
       "      <td>0.092031</td>\n",
       "      <td>0.000817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  architecture  train_time  avg_time_epoch       mse   mse_std      rmse  \\\n",
       "0       L3_U16   23.917387        1.121345  0.020921  0.000388  0.144634   \n",
       "\n",
       "   rmse_std          mape     mape_std       mae   mae_std  \n",
       "0  0.001337  35257.523047  3342.083789  0.092031  0.000817  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dense_results.to_csv(f'../../evaluations/Test_Datastream_Dense.csv')\n",
    "dense_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
