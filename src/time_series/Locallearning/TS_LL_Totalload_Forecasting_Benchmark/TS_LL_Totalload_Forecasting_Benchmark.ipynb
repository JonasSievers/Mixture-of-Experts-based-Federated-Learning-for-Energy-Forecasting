{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rs1044\\Documents\\GitHub\\Mixture-of-Experts-based-Federated-Learning-for-Energy-Forecasting\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys  \n",
    "sys.path.append(\"../../../\")  \n",
    "from utils.models import *\n",
    "from utils.datahandling import *\n",
    "from utils.modelrunner import *\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "logging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Get data \u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_users \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 4\u001b[0m cwd \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mgetcwd())))))\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(cwd\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/3final_data/Final_Totalload_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mindex)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#Get data \n",
    "num_users = 30\n",
    "\n",
    "cwd = os.path.normpath(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))))\n",
    "df = pd.read_csv(cwd+'/data/3final_data/Final_Totalload_dataset.csv', index_col='Date')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Get the first date from the index\n",
    "start_date = df.index.min()\n",
    "# Calculate the end date as one year from the start date\n",
    "end_date = start_date + pd.DateOffset(years=1)\n",
    "# Filter the dataframe to only include the first year of data\n",
    "df = df[(df.index >= start_date) & (df.index < end_date)]\n",
    "\n",
    "df_array = []\n",
    "for idx in range(num_users):\n",
    "    df_array.append(df[[f'User{idx+1}', 'temp', 'rhum', 'wspd', 'PC1', 'hour sin', 'hour cos', f'User{idx+1}_lag_24hrs']])\n",
    "\n",
    "df_array[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "sequence_length = 25\n",
    "batch_size = 16\n",
    "num_features = df_array[0].shape[1]\n",
    "horizon = 1\n",
    "max_epochs = 100\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics=[\n",
    "    tf.keras.metrics.RootMeanSquaredError(), \n",
    "    tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "    tf.keras.metrics.MeanAbsoluteError(),\n",
    "]\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='min')\n",
    "timing_callback = TimingCallback()\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "callbacks=[early_stopping, timing_callback, custom_callback]\n",
    "\n",
    "#BiLSTM\n",
    "lstm_layers = 2\n",
    "lstm_units = 8\n",
    "lstm_all_results = pd.DataFrame(columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"])\n",
    "lstm_results = pd.DataFrame(columns=['architecture', 'train_time', 'avg_time_epoch', 'mse','mse_std', 'rmse','rmse_std','mape','mape_std','mae','mae_std'])\n",
    "\n",
    "#CNN\n",
    "cnn_layers = 4\n",
    "cnn_kernel_size = 1\n",
    "cnn_filter_size = 8\n",
    "cnn_dense_units = 16\n",
    "cnn_all_results = pd.DataFrame(columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"])\n",
    "cnn_results = pd.DataFrame(columns=['architecture', 'train_time', 'avg_time_epoch', 'mse','mse_std', 'rmse','rmse_std','mape','mape_std','mae','mae_std'])\n",
    "\n",
    "#Transfotransformer_architecture = \"Transformer_ED2_h4_d32\"\n",
    "transformer_layers = 2\n",
    "transformer_heads = 4\n",
    "transformer_dense_units = 16\n",
    "transformer_all_results = pd.DataFrame(columns=[\"user\", \"architecture\", \"train_time\", \"avg_time_epoch\", \"mse\", \"rmse\", \"mape\", \"mae\"])\n",
    "transformer_results = pd.DataFrame(columns=['architecture', 'train_time', 'avg_time_epoch', 'mse','mse_std', 'rmse','rmse_std','mape','mape_std','mae','mae_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, Validation and Test datasets\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "#Create Train, Validation and Test datasets\n",
    "for idx, df in enumerate(df_array):\n",
    "    n = len(df)\n",
    "    train_df = df[0:int(n*0.7)]\n",
    "    val_df = df[int(n*0.7):int(n*0.9)]\n",
    "    test_df = df[int(n*0.9):]\n",
    "\n",
    "    # Min max sclaing\n",
    "    train_df = min_max_scaling(train_df)\n",
    "    val_df = min_max_scaling(val_df)\n",
    "    test_df = min_max_scaling(test_df)\n",
    "\n",
    "    # Sequencing\n",
    "    train_sequences = create_sequences(train_df, sequence_length)\n",
    "    val_sequences = create_sequences(val_df, sequence_length)\n",
    "    test_sequences = create_sequences(test_df, sequence_length)\n",
    "\n",
    "    #Split into feature and label\n",
    "    X_train[f'user{idx+1}'], y_train[f'user{idx+1}'] = prepare_data(train_sequences, batch_size)\n",
    "    X_val[f'user{idx+1}'], y_val[f'user{idx+1}'] = prepare_data(val_sequences, batch_size)\n",
    "    X_test[f'user{idx+1}'], y_test[f'user{idx+1}'] = prepare_data(test_sequences, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----User:  1\n",
      "Round:  0\n"
     ]
    }
   ],
   "source": [
    "run_bilstm_model(\n",
    "    wb_project_name = \"TS_LL_Totalload_Forecasting_Benchmark\",\n",
    "    wb_model_name = \"bilstm\",\n",
    "    wb_project = \"TS_LL_Totalload\",\n",
    "    save_path = os.getcwd(),\n",
    "    df_array = df_array,\n",
    "    max_epochs = max_epochs,\n",
    "    batch_size = batch_size,\n",
    "    X_train = X_train,\n",
    "    horizon = horizon, \n",
    "    lstm_layers = lstm_layers, \n",
    "    lstm_units = lstm_units,    \n",
    "    metrics = metrics,\n",
    "    loss = loss,\n",
    "    y_train = y_train,\n",
    "    X_val = X_val,\n",
    "    y_val = y_val,\n",
    "    X_test = X_test,\n",
    "    y_test = y_test,\n",
    "    callbacks = callbacks,\n",
    "    results = lstm_results,\n",
    "    all_results = lstm_all_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cnn_model(\n",
    "    wb_project_name = \"TS_LL_Totalload_Forecasting_Benchmark\",\n",
    "    wb_model_name = \"cnn\",\n",
    "    wb_project = \"TS_LL_Totalload\",\n",
    "    save_path = os.getcwd(),\n",
    "    df_array = df_array,\n",
    "    max_epochs = max_epochs,\n",
    "    batch_size = batch_size,\n",
    "    X_train = X_train,\n",
    "    horizon = horizon, \n",
    "    cnn_layers = cnn_layers, \n",
    "    cnn_filter_size = cnn_filter_size,\n",
    "    cnn_kernel_size = cnn_kernel_size, \n",
    "    cnn_dense_units = cnn_dense_units,      \n",
    "    metrics = metrics,\n",
    "    loss = loss,\n",
    "    y_train = y_train,\n",
    "    X_val = X_val,\n",
    "    y_val = y_val,\n",
    "    X_test = X_test,\n",
    "    y_test = y_test,\n",
    "    callbacks = callbacks,\n",
    "    results = cnn_results,\n",
    "    all_results = cnn_all_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_transformer_model(\n",
    "     wb_project_name = \"TS_LL_Totalload_Forecasting_Benchmark\",\n",
    "    wb_model_name = \"transformer\",\n",
    "    wb_project = \"TS_LL_Totalload\",\n",
    "    save_path = os.getcwd(),\n",
    "    df_array = df_array,\n",
    "    max_epochs = max_epochs,\n",
    "    batch_size = batch_size,\n",
    "    X_train = X_train,\n",
    "    horizon = horizon, \n",
    "    sequence_length = sequence_length, \n",
    "    transformer_layers = transformer_layers, \n",
    "    num_features = num_features, \n",
    "    transformer_heads = transformer_heads, \n",
    "    transformer_dense_units = transformer_dense_units,  \n",
    "    metrics = metrics,\n",
    "    loss = loss,\n",
    "    y_train = y_train,\n",
    "    X_val = X_val,\n",
    "    y_val = y_val,\n",
    "    X_test = X_test,\n",
    "    y_test = y_test,\n",
    "    callbacks = callbacks,\n",
    "    results = transformer_results,\n",
    "    all_results = transformer_all_results\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
